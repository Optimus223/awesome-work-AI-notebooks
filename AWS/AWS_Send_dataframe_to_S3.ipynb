{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"10%\" alt=\"Naas\" src=\"https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png?w=160\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS - Send dataframe to S3\n",
    "<a href=\"https://app.naas.ai/user-redirect/naas/downloader?url=https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/AWS/AWS_Get_files_from_S3_bucket.ipynb\" target=\"_parent\"><img src=\"https://naasai-public.s3.eu-west-3.amazonaws.com/open_in_naas.svg\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tags:** #aws #cloud #storage #S3bucket #operations #snippet #dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** [Maxime Jublou](https://www.linkedin.com/in/maximejublou/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : [AWS Data Wrangler](https://github.com/awslabs/aws-data-wrangler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import awswrangler as wr\n",
    "except:\n",
    "    !pip install awswrangler --user\n",
    "    import awswrangler as wr\n",
    "import pandas as pd\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credentials\n",
    "AWS_ACCESS_KEY_ID = \"YOUR_AWS_ACCESS_KEY_ID\"\n",
    "AWS_SECRET_ACCESS_KEY = \"YOUR_AWS_SECRET_ACCESS_KEY\"\n",
    "AWS_DEFAULT_REGION = \"YOUR_AWS_DEFAULT_REGION\"\n",
    "\n",
    "# Bucket\n",
    "BUCKET_PATH = f\"s3://naas-data-lake/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T06:42:00.638182Z",
     "iopub.status.busy": "2022-04-21T06:42:00.637939Z",
     "iopub.status.idle": "2022-04-21T06:42:00.843677Z",
     "shell.execute_reply": "2022-04-21T06:42:00.842680Z",
     "shell.execute_reply.started": "2022-04-21T06:42:00.638157Z"
    },
    "tags": []
   },
   "source": [
    "### Setup Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID\n",
    "%env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY\n",
    "%env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": [1, 2],\n",
    "    \"value\": [\"foo\", \"boo\"],\n",
    "    \"date\": [date(2020, 1, 1), date(2020, 1, 2)]\n",
    "})\n",
    "\n",
    "# Display dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send dataset to S3\n",
    "Wrangler has 3 different write modes to store Parquet Datasets on Amazon S3.\n",
    "- **append** (Default) : Only adds new files without any delete.\n",
    "- **overwrite** : Deletes everything in the target directory and then add new files.\n",
    "- **overwrite_partitions** (Partition Upsert) : Only deletes the paths of partitions that should be updated and then writes the new partitions files. It's like a \"partition Upsert\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=BUCKET_PATH,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
