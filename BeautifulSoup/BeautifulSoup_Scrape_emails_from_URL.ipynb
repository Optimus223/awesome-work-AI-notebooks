{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "naas-logo",
   "metadata": {
    "papermill": {},
    "tags": [
     "logo"
    ]
   },
   "source": [
    "<img width=\"10%\" alt=\"Naas\" src=\"https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png?w=160\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0491b7b-b0f5-4cf5-a7b5-6eb5c2ab8e9f",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "# BeautifulSoup - Scrape emails from URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740118ad-0b5f-4e5c-ad06-e817fb704561",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "**Tags:** #beautifulsoup #python #scraping #emails #url #webscraping #html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe8be2-5222-4691-8e3b-e4b6e2925dfb",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "**Author:** [Florent Ravenel](https://www.linkedin.com/in/florent-ravenel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65716fb7-4374-4443-84a1-7e0fa9c49679",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "**Description:** This notebook will show how to scrape emails stored in HTML webpage using BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd681af1-c53f-4bfc-9f32-c53f0cbf2dd2",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "<u>References:</u>\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Regular Expression Documentation](https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9ce69-4f8e-4415-b0a9-65fae43fa9fb",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ab0c4-d4b8-4600-b552-ed535c5ec536",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e531a0d4-8269-44ee-97e1-dda6e0f74754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T18:45:03.381643Z",
     "iopub.status.busy": "2023-02-16T18:45:03.381371Z",
     "iopub.status.idle": "2023-02-16T18:45:03.868715Z",
     "shell.execute_reply": "2023-02-16T18:45:03.868006Z",
     "shell.execute_reply.started": "2023-02-16T18:45:03.381563Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3bd3c-e550-4dd4-8aeb-64ed61a39049",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "### Setup Variables\n",
    "- `url`: URL of the webpage to scrape\n",
    "- `limit`: number of emails found to stop scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c3dccd-4e21-48d5-bbe0-786e87f40d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T18:45:03.869956Z",
     "iopub.status.busy": "2023-02-16T18:45:03.869738Z",
     "iopub.status.idle": "2023-02-16T18:45:03.875075Z",
     "shell.execute_reply": "2023-02-16T18:45:03.874573Z",
     "shell.execute_reply.started": "2023-02-16T18:45:03.869927Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://www.naas.ai/\"\n",
    "limit = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02071341-afb2-4e70-b233-7fc2797b0a79",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f2937-e5a1-4373-a2cb-0e28a304c25e",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "### Scrape emails from URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100cd4f-2938-486a-81d1-2d5645c4a93d",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "We will use the `requests` library to get the HTML content of the webpage and the `BeautifulSoup` library to parse the HTML content. We will use a regular expression to extract the emails from the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525068c0-ccb4-4941-9fd0-fb5ce306bcb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T18:45:03.876206Z",
     "iopub.status.busy": "2023-02-16T18:45:03.875987Z",
     "iopub.status.idle": "2023-02-16T18:45:04.716202Z",
     "shell.execute_reply": "2023-02-16T18:45:04.715520Z",
     "shell.execute_reply.started": "2023-02-16T18:45:03.876178Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling URL: https://www.infomerics.com/\n",
      "{'hrd@infomerics.com', 'vma@infomerics.com', 'info@infomerics.com'}\n"
     ]
    }
   ],
   "source": [
    "unscraped = deque([url])  \n",
    "\n",
    "scraped = set()  \n",
    "\n",
    "emails = set()  \n",
    "\n",
    "while len(unscraped):\n",
    "    url = unscraped.popleft()  \n",
    "    scraped.add(url)\n",
    "\n",
    "    parts = urlsplit(url)\n",
    "        \n",
    "    base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "    if '/' in parts.path:\n",
    "        path = url[:url.rfind('/')+1]\n",
    "    else:\n",
    "        path = url\n",
    "\n",
    "    print(\"Crawling URL: %s\" % url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
    "        continue\n",
    "        \n",
    "    exclude = [\"google.com\", \"gmail.com\", \"example.com\"]    \n",
    "    # Get emails from URL\n",
    "    new_emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.+[a-z]{1,3}\", url)\n",
    "    for email in new_emails:\n",
    "        for e in exclude:\n",
    "            if not email.endswith(e):\n",
    "                emails.update([email])\n",
    "                \n",
    "    # Get emails from content\n",
    "    new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.+[a-z]{1,3}\", response.text, re.I))\n",
    "    for email in new_emails:\n",
    "        for e in exclude:\n",
    "            if not email.endswith(e):\n",
    "                emails.update([email])\n",
    "                \n",
    "    if len(emails) >= limit:\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    for anchor in soup.find_all(\"a\"):\n",
    "        if \"href\" in anchor.attrs:\n",
    "            link = anchor.attrs[\"href\"]\n",
    "        else:\n",
    "            link = ''\n",
    "\n",
    "        if link.startswith('/'):\n",
    "            link = base_url + link\n",
    "        \n",
    "        elif not link.startswith('http'):\n",
    "            link = path + link\n",
    "\n",
    "        if not link.endswith(\".gz\"):\n",
    "            if not link in unscraped and not link in scraped:\n",
    "                unscraped.append(link)\n",
    "\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c12fd0-e0cb-4b7f-994a-5f339e14b797",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9b6a7-0275-4b4a-ae3e-7627282110f4",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    "### Display result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e99b77-df31-44f7-a4a0-816ff1a3d8d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T18:45:04.717566Z",
     "iopub.status.busy": "2023-02-16T18:45:04.717291Z",
     "iopub.status.idle": "2023-02-16T18:45:04.721547Z",
     "shell.execute_reply": "2023-02-16T18:45:04.720891Z",
     "shell.execute_reply.started": "2023-02-16T18:45:04.717535Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ 3 founded on https://www.infomerics.com/\n",
      "{'hrd@infomerics.com', 'vma@infomerics.com', 'info@infomerics.com'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸš€ {len(emails)} founded on {url}\")\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28cae3-df60-4916-8ff9-1523ed012e49",
   "metadata": {
    "papermill": {},
    "tags": [
     ""
    ]
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
